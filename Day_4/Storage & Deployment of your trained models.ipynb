{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So you've trained a model....now what?\n",
    "\n",
    "This notebook will walkthrough saving your trained model so you can use it for prediction at a later data without retraining it again i.e. long after you've shutdown your current python kernel. \n",
    "\n",
    "## Reference our Data Science Workflow:\n",
    "You'll see we're in what's called **ML Engineering** which at many companies is an entirely seperate role from the person who designs & trains the model initially. People in this role tend to come from a software engineering background rather than an analytical background like a data scientist. However, there are always exceptions to the rule, and this diagram is just to give you a general idea of how this workflow happens.\n",
    "<img src='https://storage.googleapis.com/gweb-cloudblog-publish/images/Intro_To_Data_Science_Fhkolds.max-900x900.jpg' />\n",
    "\n",
    "\n",
    "\n",
    "## Example Model Deployment Process below on your local machine\n",
    "\n",
    "We won't be connecting to an external registry here, but this gives you a general idea of the flow:\n",
    "\n",
    "> 1. Store your trained model in pickle object and save it to your local folder (we would always store our model as some object/file we can use later)\n",
    "> 1. Read this file into a new machine to create an instance of the model with its trained weights. \n",
    "> 1. Run predictions from your new instance\n",
    "> 1. Use those predictions to take action or store those predictions in a database or file to use later\n",
    "\n",
    "In production settings, you would use your model instance in 1 of 2 use cases:\n",
    "> 1. **Batch Processing:** Using an orchestrator (e.g. cron, airflow) you would start a computer on some schedule and then run your python code on that computer (e.g. `python my_prediction_job.py`)<br/>\n",
    "The script would look something like this:<br/>\n",
    "```python\n",
    " import pandas as pd \n",
    " from sklearn import svm\n",
    " from joblib import load\n",
    " new_data_to_make_predictions = pd.read_csv('new_data_file.csv') # read my new data\n",
    " feature_cols = ['feature_1', 'feature_2']\n",
    " X_unseen = new_data_to_make_predictions[feature_cols] #specify the features\n",
    " model = load('my_model.joblib') #load my model from storage\n",
    " y_preds = model.predict(X_unseen) #run predictions\n",
    " new_data_to_make_predictions['predicted_y'] = y_preds\n",
    " new_data_to_make_predictions.to_csv('preds.csv')#write predictions to a file for future use```\n",
    "> 1. **Live Inference:** an always on app that you can communicate with through an API or event streaming for real-time predictions. You would load the model from this file (or a file similar to it) into an always on computer (e.g. an ec2 instance, group of instances, containers in kubernetes, etc.). It would be ready to recieve a request that contained a set of features and would return a response with your prediction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T22:53:48.747458Z",
     "start_time": "2024-01-11T22:53:48.673138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my_model.joblib']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "clf = svm.SVC()\n",
    "X, y= datasets.load_iris(return_X_y=True)\n",
    "clf.fit(X, y)\n",
    "\n",
    "from joblib import dump, load\n",
    "dump(clf, 'my_model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T22:53:52.558499Z",
     "start_time": "2024-01-11T22:53:52.529470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_new_X, _ = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "clf2 = load('my_model.joblib') \n",
    "y_preds = clf2.predict(some_new_X[0:1])\n",
    "y_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T22:54:33.000384Z",
     "start_time": "2024-01-11T22:54:32.973568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = clf.predict(some_new_X[0:1])\n",
    "y_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T22:54:43.475234Z",
     "start_time": "2024-01-11T22:54:43.447810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the feautres we \n",
    "some_new_X[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T22:55:03.134844Z",
     "start_time": "2024-01-11T22:55:03.103691Z"
    }
   },
   "outputs": [],
   "source": [
    "y_preds.tofile('my_preds_file.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Inference App Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T21:59:41.181309Z",
     "start_time": "2024-01-11T21:59:40.632670Z"
    }
   },
   "source": [
    "Example App that could be deployed on a remote server:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flask import Flask, request, jsonify\n",
    "from joblib import load\n",
    "from sklearn import svm\n",
    "\n",
    "app = Flask(__name__)\n",
    "clf3 = load('my_model.joblib')\n",
    "\n",
    "@app.route('/api',methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json(force=True)\n",
    "    df = pd.io.json.json_normalize(data)\n",
    "    feature_cols = ['feature_1', 'feature_2', 'feature_3', 'feature_4']\n",
    "    df['feature_1'] = df['feature_1'].astype(float)\n",
    "    df['feature_2'] = df['feature_2'].astype(float)\n",
    "    df['feature_3'] = df['feature_3'].astype(float)\n",
    "    df['feature_4'] = df['feature_4'].astype(float)\n",
    "    prediction = clf3.predict(df[feature_cols])\n",
    "    output = str(prediction[0])\n",
    "    return jsonify(output)\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5009, debug=True)```\n",
    "    \n",
    "This would be in a file called something like `server.py` and be started from the terminal with a command like: ``` python server.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T23:04:01.483674Z",
     "start_time": "2024-01-11T23:04:01.444968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:5010/api\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'feature_1': '5.1',\n",
       " 'feature_2': '3.5',\n",
       " 'feature_3': '1.4',\n",
       " 'feature_4': '0.2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://localhost:5010/api'\n",
    "new_observation = {'feature_1':'5.1','feature_2':'3.5','feature_3':'1.4','feature_4':'0.2'}\n",
    "print(url)\n",
    "display(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T23:05:04.246737Z",
     "start_time": "2024-01-11T23:05:04.210752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.post(url,json=new_observation)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T23:05:55.686102Z",
     "start_time": "2024-01-11T23:05:55.656540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(r.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
